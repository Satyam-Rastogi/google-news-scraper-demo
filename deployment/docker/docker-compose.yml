version: '3.8'

services:
  news-scraper-api:
    build:
      context: ../..
      dockerfile: deployment/docker/Dockerfile
    container_name: news-scraper-api
    ports:
      - "8000:8000"
    environment:
      - HOST=0.0.0.0
      - PORT=8000
      - DEBUG=false
      - LOG_LEVEL=INFO
      - API_PREFIX=/api/v1
      - ENABLE_VERSIONING=true
      - DEFAULT_MAX_RESULTS=50
      - MAX_RESULTS_LIMIT=100
      - REQUEST_TIMEOUT=30
      - REQUEST_DELAY=1.0
      - DEFAULT_OUTPUT_FORMAT=json
      - OUTPUT_DIRECTORY=artifacts/data/scraped
      - LOGS_DIRECTORY=logs
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_PASSWORD=
      - ENABLE_CACHE=true
      - CACHE_TTL=3600
    volumes:
      - news_data:/app/artifacts
      - news_logs:/app/logs
    depends_on:
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - news-scraper-network

  # Celery worker for news scraping tasks
  celery-worker:
    build:
      context: ../..
      dockerfile: deployment/docker/Dockerfile
    container_name: news-scraper-celery-worker
    command: celery -A src.workers.celery_app:celery_app worker --loglevel=info --queues=news_scraping --concurrency=2 --hostname=news-worker@%h
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_PASSWORD=
      - LOG_LEVEL=INFO
      - OUTPUT_DIRECTORY=artifacts/data/scraped
      - LOGS_DIRECTORY=logs
    volumes:
      - news_data:/app/artifacts
      - news_logs:/app/logs
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - news-scraper-network

  # Celery cleanup worker
  celery-cleanup-worker:
    build:
      context: ../..
      dockerfile: deployment/docker/Dockerfile
    container_name: news-scraper-celery-cleanup
    command: celery -A src.workers.celery_app:celery_app worker --loglevel=info --queues=cleanup --concurrency=1 --hostname=cleanup-worker@%h
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_PASSWORD=
      - LOG_LEVEL=INFO
      - OUTPUT_DIRECTORY=artifacts/data/scraped
      - LOGS_DIRECTORY=logs
    volumes:
      - news_data:/app/artifacts
      - news_logs:/app/logs
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - news-scraper-network

  # Celery beat scheduler
  celery-beat:
    build:
      context: ../..
      dockerfile: deployment/docker/Dockerfile
    container_name: news-scraper-celery-beat
    command: celery -A src.workers.celery_app:celery_app beat --loglevel=info
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_PASSWORD=
      - LOG_LEVEL=INFO
      - OUTPUT_DIRECTORY=artifacts/data/scraped
      - LOGS_DIRECTORY=logs
    volumes:
      - news_data:/app/artifacts
      - news_logs:/app/logs
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - news-scraper-network

  # Flower monitoring
  celery-flower:
    build:
      context: ../..
      dockerfile: deployment/docker/Dockerfile
    container_name: news-scraper-flower
    command: celery -A src.workers.celery_app:celery_app flower --port=5555
    ports:
      - "5555:5555"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_PASSWORD=
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - news-scraper-network

  # Redis for caching and session management
  redis:
    image: redis:7-alpine
    container_name: news-scraper-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - news-scraper-network

  # Optional: Add a reverse proxy
  nginx:
    image: nginx:alpine
    container_name: news-scraper-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - news-scraper-api
      - redis
    restart: unless-stopped
    networks:
      - news-scraper-network

volumes:
  news_data:
    driver: local
  news_logs:
    driver: local
  redis_data:
    driver: local

networks:
  news-scraper-network:
    driver: bridge
